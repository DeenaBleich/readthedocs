******************
BigQuery Tips & Tricks: 
******************

Last month, we transformed a typical genomics file type (the vcf file format) into a BigQuery table. This month, we’ll continue exploring how to load data into bigquery tables. Because genomics files are often very large in size, we'll also explore some tricks on how to partition tables to query to save both money and time!

**I. Loading CSV Data into BigQuery**

First, let's explore how to load csv files into big query. Here’s an example of a common use-case:

*A lab with which your group collaborates is generating large amounts of RNAseq gene expression data. They have been following the nice analyses your group has done using BigQuery and would like you to help them perform similar analyses. Thus far, they have saved all of their RNAseq expression data into CSV format. They need your help first loading their RNAseq data files into BigQuery.*

Here, we’ll learn how to load CSV files into BigQuery tables. We’ll accomplish this with some very useful bq command-line tools and arguments.

Before starting:

-- This tutorial assumes that you’ve already created a GCP project. If you don’t already have one, instructions on how to set up one up can be found: `here <https://isb-cancer-genomics-cloud.readthedocs.io/en/latest/sections/HowToGetStartedonISB-CGC.html>`__

-- Ensure that you have Google Cloud SDK installed

*Loading CSV into Google CLoud Storage*  

You can load local files as well as files from Google Cloud Storage (GCS). For this exercise, let’s make a GCS bucket to store our CSV files.

1. Make a bucket to store the RNAseq CSV
::

	gsutil mb gs://RNAseq_CSVs



2. Copy the CSVs into your newly created storage bucket
::

	gsutil cp *.csv gs://RNAseq_CSVs
  
*Creating a BigQuery dataset*  

Creating tables and loading data via the BigQuery web-UI is good if
you’re only loading a small amount of data. It can be a tediously manual
process though if you have more than a handful of files. We can create
tables and load data using Google SDK’s handy bq tool. bq is a
python-based, command-line tool for BigQuery. `https://cloud.google.com/bigquery/docs/bq-command-line-tool <https://cloud.google.com/bigquery/docs/bq-command-line-tool>`__  

Let’s create a dataset that will hold our RNAseq data:
::

	bq mk RNAseq\_data
  
If successful, you will get the following message: 
::

	Dataset 'Your_Project_ID:RNAseq\_data' successfully created
  
You can also list all of the datasets associated with your project using
the following command:
::

	bq ls
  
*Generate schema for BigQuery table*
A schema for a table is a description of its field names and types.
BigQuery allows you to specify a table's schema when you load data into
a table. We can create a schema file in JSON format. You can find a
Python script (**createSchema.py**) to create a JSON schema for your
table in our github examples-Python repository.

`https://github.com/isb-cgc/examples-Python/tree/master/python <https://github.com/isb-cgc/examples-Python/tree/master/python>`__

Usage: python createSchema.py <input-filename> <nSkip>
where nSkip specifies the # of lines skipped between lines that are
parsed and checked for data-types; if the input file is small, you can
leave set nSkip to be small, but if the input file is very large, nSkip
should probably be 1000 or more (default value is 1000)

*Loading Data in BQ*

With the JSON schema file, we are now ready to load data into BigQuery.The bq load command is used to load data in BigQuery via the
command-line.

::

    bq load \\

    --source\_format=CSV \\

    --skip\_leading\_rows=1 \\

    TEMP_LOCATION=gs://path_to_a_temp_folder

    RNAseq_data.expressionFile \\

    gs://RNAseq\_CSVs/ExpressionDataTable.csv \\   

    ExpressionDataTable.csv.json  

    
You can verify that the table loaded by showing the table properties
with this command:

::

    bq show RNAseq_data.expressionFile
    

**II. BigQuery Table Clusters**
The costs of using BigQuery center around how much of a table is read by
the query. So, the same query applied to a small table versus a very
large table will incur very different costs. It simply costs more to
query a large table! In the past, we broke tables into many subtables to
save costs and time. This was the case with the methylation tables where
the entire thing consisted of 3.9 Billion rows (932 GB)! It's pretty
expensive to query that table, so we broke it into many tables by
chromosome. OK, but not entirely convenient to work with.

Now, there's a fairly simple step to accomplish the same thing,
resulting in huge cost savings without changing your SQL or table
schema! They're called 'clustered tables', which groups rows of your
BigQuery table so that your query only reads the appropriate portions of
your table. This means you can specify the cluster to be over
chromosomes, and your query will only read the portion of the table
associated with that chromosome.

 
